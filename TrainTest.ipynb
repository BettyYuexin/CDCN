{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47823305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:43:32.157441Z",
     "start_time": "2023-04-29T11:43:22.770343Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse,os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.CDCNs import Conv2d_cd, CDCN, CDCNpp\n",
    "from Load_OULUNPU_train import Spoofing_train, Normaliztion, ToTensor, RandomHorizontalFlip, Cutout, RandomErasing\n",
    "from Load_OULUNPU_valtest import Spoofing_valtest, Normaliztion_valtest, ToTensor_valtest\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pdb\n",
    "from utils import AvgrageMeter, accuracy, performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1adb2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:43:32.210008Z",
     "start_time": "2023-04-29T11:43:32.185829Z"
    }
   },
   "outputs": [],
   "source": [
    "train_image_dir = '/mnt/hdd.user/datasets/Oulu-NPU/Train_files/'            \n",
    "val_image_dir = '/mnt/hdd.user/datasets/Oulu-NPU/Dev_files/'   \n",
    "test_image_dir = '/mnt/hdd.user/datasets/Oulu-NPU/Test_files/'   \n",
    "# 文件夹内包含原来的视频.avi文件和其中每一帧的图像1_1_01_1_frame92.jpg\n",
    "# 以及每一帧的ROi region of interest在.txt文件中 文件内容类似0,455,779,638,756\n",
    "\n",
    "map_dir = '/mnt/hdd.user/datasets/Oulu-NPU/Train_depth/'   \n",
    "val_map_dir = '/mnt/hdd.user/datasets/Oulu-NPU/Dev_depth/'   \n",
    "test_map_dir = '/mnt/hdd.user/datasets/Oulu-NPU/Test_depth/' \n",
    "# 文件名1_2_11_3_frame140_depth.jpg\n",
    "\n",
    "train_list = '/mnt/hdd.user/datasets/Oulu-NPU/Protocols/Protocol_1/Train.txt'\n",
    "val_list = '/mnt/hdd.user/datasets/Oulu-NPU/Protocols/Protocol_1/Dev.txt'\n",
    "test_list =  '/mnt/hdd.user/datasets/Oulu-NPU/Protocols/Protocol_1/Test.txt'\n",
    "\n",
    "device = torch.device(\"cuda:8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca5f6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:43:32.344527Z",
     "start_time": "2023-04-29T11:43:32.212237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batchsize=7, echo_batches=50, epochs=1400, gamma=0.5, gpu=1, log='CDCNpp_test', lr=0.0001, step_size=500)\n"
     ]
    }
   ],
   "source": [
    "def contrast_depth_conv(input):\n",
    "    ''' compute contrast depth in both of (out, label) '''\n",
    "    '''\n",
    "        input  32x32\n",
    "        output 8x32x32\n",
    "    '''\n",
    "\n",
    "    # [8, 3, 3]\n",
    "    kernel_filter_list =[\n",
    "                        [[1,0,0],[0,-1,0],[0,0,0]], [[0,1,0],[0,-1,0],[0,0,0]], [[0,0,1],[0,-1,0],[0,0,0]],\n",
    "                        [[0,0,0],[1,-1,0],[0,0,0]], [[0,0,0],[0,-1,1],[0,0,0]],\n",
    "                        [[0,0,0],[0,-1,0],[1,0,0]], [[0,0,0],[0,-1,0],[0,1,0]], [[0,0,0],[0,-1,0],[0,0,1]]\n",
    "                        ]\n",
    "    \n",
    "    kernel_filter = np.array(kernel_filter_list, np.float32)\n",
    "\n",
    "    kernel_filter = torch.from_numpy(kernel_filter.astype(np.float)).float().to(device)\n",
    "    # weights (in_channel, out_channel, kernel, kernel)\n",
    "    kernel_filter = kernel_filter.unsqueeze(dim=1) # [1, 8, 3, 3]\n",
    "    # [32, 32]--[1, 32, 32]--[8,32,32](进行完全拷贝的维度扩展)\n",
    "    input = input.unsqueeze(dim=1).expand(input.shape[0], 8, input.shape[1],input.shape[2])\n",
    "    # 输出[8,32,32]，group=8相当于每个channel分别和对应的channel的kernel进行卷积运算，得到输出对应的channel\n",
    "    contrast_depth = F.conv2d(input, weight=kernel_filter, groups=8)  # depthwise conv\n",
    "\n",
    "    return contrast_depth\n",
    "\n",
    "\n",
    "class Contrast_depth_loss(nn.Module):    # Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss\n",
    "    def __init__(self):\n",
    "        super(Contrast_depth_loss,self).__init__()\n",
    "        return\n",
    "    def forward(self, out, label): \n",
    "        '''\n",
    "        compute contrast depth in both of (out, label),\n",
    "        then get the loss of them\n",
    "        tf.atrous_convd match tf-versions: 1.4\n",
    "        '''\n",
    "        # 根据out和label分别构造depth_conv，之后计算mseloss\n",
    "        contrast_out = contrast_depth_conv(out)\n",
    "        contrast_label = contrast_depth_conv(label)\n",
    "        \n",
    "        \n",
    "        criterion_MSE = nn.MSELoss().to(device)\n",
    "    \n",
    "        loss = criterion_MSE(contrast_out, contrast_label)\n",
    "        #loss = torch.pow(contrast_out - contrast_label, 2)\n",
    "        #loss = torch.mean(loss)\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=1, help='the gpu id used for predict')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')  \n",
    "parser.add_argument('--batchsize', type=int, default=7, help='initial batchsize')  \n",
    "parser.add_argument('--step_size', type=int, default=500, help='how many epochs lr decays once')  # 500 \n",
    "parser.add_argument('--gamma', type=float, default=0.5, help='gamma of optim.lr_scheduler.StepLR, decay of lr')\n",
    "parser.add_argument('--echo_batches', type=int, default=50, help='how many batches display once')  # 50\n",
    "parser.add_argument('--epochs', type=int, default=1400, help='total training epochs')\n",
    "parser.add_argument('--log', type=str, default=\"CDCNpp_test\", help='log and save model name')\n",
    "# parser.add_argument('--finetune', action='store_true', default=False, help='whether finetune other models')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "isExists = os.path.exists(args.log)\n",
    "if not isExists:\n",
    "    os.makedirs(args.log)\n",
    "log_file = open(args.log+'/'+ args.log+'_log_P1.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636f8055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:03.149654Z",
     "start_time": "2023-04-29T11:43:32.346779Z"
    }
   },
   "outputs": [],
   "source": [
    "echo_batches = args.echo_batches\n",
    "log_file.write('Oulu-NPU, P1:\\n ')\n",
    "log_file.flush()\n",
    "\n",
    "model = CDCNpp(basic_conv=Conv2d_cd, theta=0.7).to(device)\n",
    "model.load_state_dict(torch.load(\"./CDCNpp_P1/CDCNpp_P1_299.pkl\"))\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.00005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "criterion_absolute_loss = nn.MSELoss().to(device)\n",
    "criterion_contrastive_loss = Contrast_depth_loss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6cb2819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:03.196819Z",
     "start_time": "2023-04-29T11:44:03.162735Z"
    }
   },
   "outputs": [],
   "source": [
    "#     val_threshold, test_threshold, val_ACC, val_ACER, test_ACC, test_APCER, test_BPCER, test_ACER, test_ACER_test_threshold = performances(map_score_val_filename, map_score_test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c308cfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:03.822672Z",
     "start_time": "2023-04-29T11:44:03.198857Z"
    }
   },
   "outputs": [],
   "source": [
    "# map_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b847ea27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:09.093482Z",
     "start_time": "2023-04-29T11:44:03.824790Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.0014561937423422933\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ###########################################\n",
    "    '''                val             '''\n",
    "    ###########################################\n",
    "    # val for threshold\n",
    "    val_data = Spoofing_valtest(val_list, val_image_dir, val_map_dir, transform=transforms.Compose([Normaliztion_valtest(), ToTensor_valtest()]))\n",
    "    dataloader_val = DataLoader(val_data, batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "    \n",
    "    map_score_list = []\n",
    "    cnt = 0\n",
    "    for i, sample_batched in enumerate(dataloader_val):\n",
    "        inputs, spoof_label = sample_batched['image_x'].to(device), sample_batched['spoofing_label'].to(device)\n",
    "        val_maps = sample_batched['val_map_x'].to(device)   # binary map from PRNet\n",
    "        \n",
    "        map_score = 0.0\n",
    "        for frame_t in range(inputs.shape[1]):\n",
    "            map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs[:,frame_t,:,:,:])\n",
    "            \n",
    "            score_norm = torch.sum(map_x)/torch.sum(val_maps[:,frame_t,:,:])\n",
    "            map_score += score_norm\n",
    "        map_score = map_score/inputs.shape[1]\n",
    "        print(f\"{i}:{map_score}\")\n",
    "        cnt += 1\n",
    "        break\n",
    "        if map_score == np.inf or map_score == np.nan:\n",
    "            break\n",
    "        if cnt > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebf27c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:10.435311Z",
     "start_time": "2023-04-29T11:44:09.096999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]['image_x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9bdffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:10.454279Z",
     "start_time": "2023-04-29T11:44:10.449558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 3, 256, 256])\n",
      "torch.Size([1, 8, 32, 32])\n",
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(val_maps.shape)\n",
    "print(map_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f167ea05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:11.245353Z",
     "start_time": "2023-04-29T11:44:10.461401Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68.2005, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(38637., device='cuda:8')\n",
      "tensor(0.0018, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(48.8168, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(38888., device='cuda:8')\n",
      "tensor(0.0013, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(65.8934, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(38477., device='cuda:8')\n",
      "tensor(0.0017, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(49.8434, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(39888., device='cuda:8')\n",
      "tensor(0.0012, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(81.2453, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(41691., device='cuda:8')\n",
      "tensor(0.0019, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(44.3709, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(41917., device='cuda:8')\n",
      "tensor(0.0011, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(69.3602, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(42156., device='cuda:8')\n",
      "tensor(0.0016, device='cuda:8', grad_fn=<DivBackward0>)\n",
      "tensor(44.4553, device='cuda:8', grad_fn=<SumBackward0>)\n",
      "tensor(43827., device='cuda:8')\n",
      "tensor(0.0010, device='cuda:8', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for frame_t in range(inputs.shape[1]):\n",
    "    map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs[:,frame_t,:,:,:])\n",
    "    print(torch.sum(map_x))\n",
    "    print(torch.sum(val_maps[:,frame_t,:,:]))\n",
    "    score_norm = torch.sum(map_x)/torch.sum(val_maps[:,frame_t,:,:])\n",
    "    print(score_norm)\n",
    "    map_score += score_norm\n",
    "map_score = map_score/inputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1b65eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T11:44:11.265739Z",
     "start_time": "2023-04-29T11:44:11.254324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [187., 185., 184.,  ...,   0.,   0.,   0.],\n",
       "         [182., 180., 176.,  ...,   0.,   0.,   0.],\n",
       "         [180., 179., 177.,  ...,   0.,   0.,   0.]]], device='cuda:8')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_maps[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38042299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T13:21:33.297037Z",
     "start_time": "2023-04-29T13:21:31.286809Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/CDCN-master/CVPR2020_paper_codes/Load_OULUNPU_train.py\", line 227, in __getitem__\n    sample = self.transform(sample)\n  File \"/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n    img = t(img)\n  File \"/workspace/CDCN-master/CVPR2020_paper_codes/Load_OULUNPU_valtest.py\", line 57, in __call__\n    image_x, val_map_x, spoofing_label = sample['image_x'],sample['val_map_x'] ,sample['spoofing_label']\nKeyError: 'val_map_x'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2afe4d821361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpoofing_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNormaliztion_valtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor_valtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspoof_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'map_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spoofing_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/CDCN-master/CVPR2020_paper_codes/Load_OULUNPU_train.py\", line 227, in __getitem__\n    sample = self.transform(sample)\n  File \"/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n    img = t(img)\n  File \"/workspace/CDCN-master/CVPR2020_paper_codes/Load_OULUNPU_valtest.py\", line 57, in __call__\n    image_x, val_map_x, spoofing_label = sample['image_x'],sample['val_map_x'] ,sample['spoofing_label']\nKeyError: 'val_map_x'\n"
     ]
    }
   ],
   "source": [
    "train_data = Spoofing_train(train_list, train_image_dir, map_dir, transform=transforms.Compose([RandomErasing(), RandomHorizontalFlip(),  ToTensor(), Cutout(), Normaliztion()]))\n",
    "dataloader_train = DataLoader(train_data, batch_size=args.batchsize, shuffle=True, num_workers=1)\n",
    "for i, sample_batched in enumerate(dataloader_train):\n",
    "    sample_batch = train_data[0]\n",
    "    inputs, map_label, spoof_label = sample_batched['image_x'].to(device), sample_batched['map_x'].to(device), sample_batched['spoofing_label'].to(device) \n",
    "    map_x, embedding, x_Block1, x_Block2, x_Block3, x_input =  model(inputs)\n",
    "    break\n",
    "\n",
    "print(inputs.shape)\n",
    "print(map_label.shape)\n",
    "print(map_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd132c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T13:17:20.028658Z",
     "start_time": "2023-04-29T13:17:19.793834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['image_x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c888885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
